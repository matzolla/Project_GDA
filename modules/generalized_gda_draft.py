# -*- coding: utf-8 -*-
"""Generalized_GDA_Draft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tZHfP6SzdZ5WLDVXM69aocIIpQnO7Q49
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

## this has been pushed by avotra ######
from PCA import PCA

data=pd.read_csv('df_td_idf1.csv')

data.shape

# from sklearn.datasets import load_iris
# data = load_iris()
# X = data.data
# Y = data.target

def treat_data(X,Y):
  """
  Input X----> an array
        Y----> an array, the labels
  Output a dataframe---->pd
  """
  data=pd.DataFrame(X)
  data['label']=Y

  return data

def split_data(df, train_percent= 0.8):
  np.random.seed(2)
  perm = np.random.permutation(df.index)

  n= len(df)
  train_index = int(train_percent * n)

  train = df.iloc[perm[:train_index]]
  test = df.iloc[perm[train_index:]]

  x_train, x_test, y_train, y_test= train.iloc[:, :-1], test.iloc[:, :-1], train.iloc[:, -1], test.iloc[:, -1]
  return pd.DataFrame(np.c_[x_train.values, y_train.values]), pd.DataFrame(np.c_[x_test.values, y_test.values])

def multinomial(data):
    """
    calculating the 1st parameters: phi=1{Y==1}/m
    input---> data
    output--> a vector of phy (for the multinomial)
    """
    N=len(data['x_17954'])
    Y=data['x_17954'].values
    classes=data['x_17954'].unique() ### the different classes
    phys=np.zeros(len(classes))

    for i in range(len(classes)):
      value=len(Y[Y==classes[i]])

      phys[i]=value/N

    return phys

def compute_mus(data):
  """
  Input data----> a data frame
  Output    ----> a matrix whose components are vectors of u
  """
  classes=data['x_17954'].unique()
  mus=np.zeros((len(classes),data.shape[1]-1))
  for i in classes:
    ##select a subset of a particular class
    X=data[data['x_17954']==i]
    X=X.values[:,:-1] ## all the X of class i
    y=X[:,-1]  ## all the class i

    m=len(y) ## in this case the m is just the count of number of element of class i
    conditional_sum_x=np.zeros(X.shape[1])

    for j in range(m):
      xi=X[j,:]

      conditional_sum_x=conditional_sum_x+xi

    #mus.append(conditional_sum_x/m)
    mus[int(i),:]=conditional_sum_x/m


  return mus

def multi_gaussian(sigma,x,mu,n):
    diff = x - mu
    inv_sigma = np.linalg.inv(sigma)
    det_sigma = np.linalg.det(sigma)
    #n = sigma.shape[1]
    px_py = (1/((np.pi)**(n/2)*np.sqrt(det_sigma)))* np.exp(-0.5* diff.T@inv_sigma@diff)

    return px_py

#data=treat_data(X,Y)

def compute_sigma(data,mus):

  """Input---> a data frame containing the input and labels
          ---> a matrix containing the different mus [mu_0,mu_1,..,mu_n]
          
     Output--> a variance-covariance matrix"""

  N=len(data['x_17954'])
  classes=data['x_17954'].unique() ## select the classes
  sigma=np.zeros((data.shape[1]-1,data.shape[1]-1)) ## initialize the covariance matrix 

  for i in range(len(classes)):
    X=data[data['x_17954']==classes[i]] ## the inputs of class i
    X=X.values[:,:-1]
    mu_i=mus[i,:].reshape((1,X.shape[1]))
    y=X[:,-1] ## the labels i
    m=len(y)
    for j in range(m):
      xi=X[j,:]
      sigm=(xi.reshape((1,X.shape[1])).T-mu_i)@(xi.reshape((1,X.shape[1])).T-mu_i).T

      sigma+=sigm

  return sigma/N

data_train,data_test=split_data(data)

data_train.columns=['x_'+str(i) for i in range(17955)]

data_test.columns=['x_'+str(i) for i in range(17955)]

def predict(data,sigma,mus):
  X=data.values[:,:-1]
  prediction=[]
  n=X.shape[0]
  for x in X:
    preds = []
    for i in range(len(mus)):
      px_py =  multi_gaussian(sigma,x,mus[i],n)
      post_i = px_py * multinomial(data)[i]

      #print(px_py)
      preds.append(post_i)
    #print(preds)  
    classes = np.array(data['x_17954'].unique())
    ypred = classes[np.argmax(preds)]
    prediction.append(ypred)

  return prediction

def accuracy(y_test,ypred):
  # out=0
  n=len(y_test)
  out = np.sum([1 if y_test[i]==ypred[i] else 0 for i in range(len(y_test))])
  return out/n

multinomial(data_train)

mus=compute_mus(data_train)

mus

sigma=compute_sigma(data_train,mus)

y_pred=predict(data,sigma,mus)

accuracy(Y,y_pred)

k = 2 # reduce the data dimensionality to 2
# X = df_frequency_encoding.iloc[:, :-1]
X = data_train.iloc[:, :-1]
# X = X.fillna(0)
z = PCA(X, k)
principal_df = pd.DataFrame(z, columns=['PC1', 'PC2'])
principal_df.head()

# Visualize the result of PCA for 2 components
import matplotlib.pyplot as plt
import seaborn as sb
plt.figure(figsize = (6,6))
sb.scatterplot(data = principal_df , x = 'PC1',y = 'PC2')